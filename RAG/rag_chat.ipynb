{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikaiming/LLM_Relevent/blob/main/RAG/rag_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaK62oX3NhRf"
      },
      "source": [
        "# 基于langchain创建自己专属的对话大模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh1ZaAnefRRy"
      },
      "source": [
        "1. 领域精准问答\n",
        "2. 数据更新频繁\n",
        "3. 生成内容可解释可追溯\n",
        "4. 数据隐私保护"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-173FmLdNhRj"
      },
      "source": [
        "通过这个例子，我们将基于`LangChain`, `OpenAI(LLM)`,  `vector DB`构建一个属于自己的LLM模型。\n",
        "\n",
        "主要使用的技术————***Retrieval Augmented Generation (RAG)***\n",
        "\n",
        "首先确保自己拥有一个 `OpenAI API key` (也并非必须)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzzHZ2sLNhRk"
      },
      "source": [
        "### 准备环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAivifIqNhRk"
      },
      "outputs": [],
      "source": [
        "! pip install -qU \\\n",
        "    langchain==0.0.316 \\\n",
        "    openai==0.28.1  \\\n",
        "    tiktoken==0.5.1  \\\n",
        "    cohere \\\n",
        "    chromadb==0.4.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eg0ay3TNhRm"
      },
      "source": [
        "### 创建一个对话模型(no RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yd3OUQIWNhRm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxQlUl1Cwom"
      },
      "source": [
        "OpenAI Python 的例子\n",
        "```python\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Orange.\"},\n",
        "]\n",
        "```\n",
        "https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models\n",
        "\n",
        "\n",
        "但是langchain 需要使用以下的格式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jbPKtHYsNhRn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Knock knock.\"),\n",
        "    AIMessage(content=\"Who's there?\"),\n",
        "    HumanMessage(content=\"Orange\"),\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11iDGkWPNhRn",
        "outputId": "b7cef2bb-7755-4f73-fe93-ab52360d9d30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Orange who?')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "res = chat(messages)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7db28OOMjig"
      },
      "source": [
        "因为 `res`也是`AIMessage`属性，所以我们可以直接进行添加，即可实现下一次的响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85dW12laNLmO",
        "outputId": "71af5759-5f2b-4c11-cfa4-b5132831f7a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orange you glad I'm here to assist you?\n"
          ]
        }
      ],
      "source": [
        "messages.append(res)\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YtYH-1oNhRo",
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "#### 处理LLM存在的缺陷\n",
        "1. 容易出现幻觉\n",
        "2. 信息滞后\n",
        "3. 专业领域深度知识匮乏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1PZxdF06NhRp"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"你是一个专业的知识助手。\"),\n",
        "    HumanMessage(content=\"你知道baichuan2模型吗？\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw-ylWZMQW5z",
        "outputId": "6e82a34a-1531-4c00-c22e-9dfc0c35ddd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "是的，我知道baichuan2模型。baichuan2模型是一个用于推荐系统的机器学习模型，它由阿里巴巴集团开发。该模型主要用于解决推荐系统中的个性化推荐问题。\n",
            "\n",
            "baichuan2模型基于深度学习和协同过滤的方法，通过分析用户的历史行为和偏好，来预测用户可能喜欢的物品。它能够处理大规模的数据，并且能够实时地进行推荐。\n",
            "\n",
            "baichuan2模型在阿里巴巴的电商平台上得到了广泛的应用，并取得了很好的效果。它能够提高用户的购物体验，增加销售额和转化率。\n",
            "\n",
            "总的来说，baichuan2模型是一个强大的推荐系统模型，能够帮助企业提供个性化的推荐服务，提高用户满意度和业务效果。\n"
          ]
        }
      ],
      "source": [
        "res = chat(messages)\n",
        "print(res.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXir07e9yKNW"
      },
      "source": [
        "chatgpt AI无法满足我们在某些特定领域的专业需求，我们可以通过知识注入的方式，利用prompt来解决这种问题："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6MTJRA2nQW_E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8fd83c54-23c4-47ed-bada-38fa70cbe9c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\\nBaichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\\nBaichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "baichuan2_information = [\n",
        "    \"Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\",\n",
        "    \"Baichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\",\n",
        "    \"Baichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(baichuan2_information)\n",
        "source_knowledge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHKTgq08NhRp",
        "outputId": "91d18c98-7a86-42ff-e4b7-d75a9a5def0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\n",
            "Baichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\n",
            "Baichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\n"
          ]
        }
      ],
      "source": [
        "print(source_knowledge)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pdgyyDx2yx8M"
      },
      "outputs": [],
      "source": [
        "query = \"你知道baichuan2模型吗？\"\n",
        "\n",
        "prompt_template = f\"\"\"基于以下内容回答问题：\n",
        "\n",
        "内容:\n",
        "{source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZwSPm_3qzWB7"
      },
      "outputs": [],
      "source": [
        "prompt = HumanMessage(\n",
        "    content=prompt_template\n",
        ")\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ujMvDfDhhU1",
        "outputId": "0b42521c-191b-47f7-8091-102b2ac2f285"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content='基于以下内容回答问题：\\n\\n内容:\\nBaichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\\nBaichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\\nBaichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\\n\\nQuery: 你知道baichuan2模型吗？')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P861bTreziWz",
        "outputId": "f58d8cdb-df44-49e0-80da-c79b524253f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "是的，我知道Baichuan 2模型。Baichuan 2是一个大规模多语言语言模型，专注于在多种语言中训练出优秀的模型。它使用了庞大的训练数据集，包含了2.6万亿个标记，相对于以往的模型，Baichuan 2提供了更丰富的数据资源。Baichuan 2不仅在通用任务上表现出色，还在特定领域的任务中展现了卓越的性能，为特定领域的应用提供了强有力的支持。\n"
          ]
        }
      ],
      "source": [
        "print(res.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Npmwyy808i6"
      },
      "source": [
        "当我们注入一些专业的知识后，模型就能够很好的回答相关问题。\n",
        "如果每一个问题都去用相关的外部知识进行增强拼接的话，那么回答的准确性就大大增加？？？？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXfTkYm01oWp"
      },
      "source": [
        "### 创建一个RAG对话模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8TTksfT2K3r"
      },
      "source": [
        "#### 1. 加载数据 （以baichuan2论文为例）\n",
        "\n",
        "   https://arxiv.org/pdf/2309.10305v2.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIVOnz1TxkZ4",
        "outputId": "1a745231-870d-48b2-833f-27d60531651f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VD-UF8z06txb"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2309.10305.pdf\")\n",
        "\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KsCy_vTs68I-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60a3f10-090b-4c4d-f92e-2ecce004c02e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\\nTao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng\\nXiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang\\nYiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu\\nBaichuan Inc.\\nAbstract\\nLarge language models (LLMs) have\\ndemonstrated remarkable performance on\\na variety of natural language tasks based\\non just a few examples of natural language\\ninstructions, reducing the need for extensive\\nfeature engineering. However, most powerful\\nLLMs are closed-source or limited in their\\ncapability for languages other than English. In\\nthis technical report, we present Baichuan 2,\\na series of large-scale multilingual language\\nmodels containing 7 billion and 13 billion\\nparameters, trained from scratch, on 2.6 trillion\\ntokens. Baichuan 2 matches or outperforms\\nother open-source models of similar size on\\npublic benchmarks like MMLU, CMMLU,\\nGSM8K, and HumanEval. Furthermore,\\nBaichuan 2 excels in vertical domains such\\nas medicine and law. We will release all\\npre-training model checkpoints to benefit the\\nresearch community in better understanding\\nthe training dynamics of Baichuan 2.\\n1 Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 2022). This increase in scale has\\nled to significant improvements in the capabilities\\nof language models, enabling more human-like\\nfluency and the ability to perform a diverse range\\nof natural language tasks. With the introduction of\\nAuthors are listed alphabetically, correspondent:\\ndaniel@baichuan-inc.com.ChatGPT (OpenAI, 2022) from OpenAI, the power\\nof these models to generate human-like text has\\ncaptured widespread public attention. ChatGPT\\ndemonstrates strong language proficiency across\\na variety of domains, from conversing casually to\\nexplaining complex concepts. This breakthrough\\nhighlights the potential for large language models\\nto automate tasks involving natural language\\ngeneration and comprehension.\\nWhile there have been exciting breakthroughs\\nand applications of LLMs, most leading LLMs like\\nGPT-4 (OpenAI, 2023), PaLM-2 (Anil et al., 2023),\\nand Claude (Claude, 2023) remain closed-sourced.\\nDevelopers and researchers have limited access to\\nthe full model parameters, making it difficult for\\nthe community to deeply study or fine-tune these\\nsystems. More openness and transparency around\\nLLMs could accelerate research and responsible\\ndevelopment within this rapidly advancing field.\\nLLaMA (Touvron et al., 2023a), a series of large\\nlanguage models developed by Meta containing up\\nto 65 billion parameters, has significantly benefited\\nthe LLM research community by being fully open-\\nsourced. The open nature of LLaMA, along with\\nother open-source LLMs such as OPT (Zhang\\net al., 2022), Bloom (Scao et al., 2022), MPT\\n(MosaicML, 2023) and Falcon (Penedo et al.,\\n2023), enables researchers to freely access the\\nmodels for examination, experimentation, and\\nfurther development. This transparency and access\\ndistinguishes LLaMA from other proprietary\\nLLMs. By providing full access, the open-source\\nLLMs have accelerated research and advances in\\nthe field, leading to new models like Alpaca (Taori\\net al., 2023), Vicuna (Chiang et al., 2023), and', metadata={'source': '/tmp/tmp10lrfiuo/tmp.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZN6yzwi7J61"
      },
      "source": [
        "#### 2. 知识切片 将文档分割成均匀的块。每个块是一段原始文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wt3G5-ph7gho"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCXqYY4D7gkp",
        "outputId": "8c2d859f-2e13-4801-afe8-516d9c46893d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(docs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgABYtKp8_Ke"
      },
      "source": [
        "#### 3. 利用embedding模型对每个文本片段进行向量化，并储存到向量数据库中"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AmI_-A1-ziZN"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "embed_model = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents=docs, embedding=embed_model , collection_name=\"openai_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMImTaLci2oJ",
        "outputId": "8981efb5-467c-4c83-9040-94e96f78502c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-fgbDgQC77h"
      },
      "source": [
        "#### 4. 通过向量相似度检索和问题最相关的K个文档。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zuoRfbU_Du3S"
      },
      "outputs": [],
      "source": [
        "query = \"How large is the baichuan2 vocabulary?\"\n",
        "result = vectorstore.similarity_search(query ,k = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dliY5xHaC2NN",
        "outputId": "149adebf-0c0c-4e79-bbd7-f4c72256aa0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='languages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).\\nWith such a massive amount of training data,', metadata={'page': 1, 'source': '/tmp/tmp10lrfiuo/tmp.pdf'}),\n",
              " Document(page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun', metadata={'page': 0, 'source': '/tmp/tmp10lrfiuo/tmp.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymUjel7-E-t1"
      },
      "source": [
        "#### 5. 原始`query`与检索得到的文本组合起来输入到语言模型，得到最终的回答"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9wBIBDnIC2P8"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str):\n",
        "  # 获取top3的文本片段\n",
        "  results = vectorstore.similarity_search(query, k=3)\n",
        "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "  # 构建prompt\n",
        "  augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "  contexts:\n",
        "  {source_knowledge}\n",
        "\n",
        "  query: {query}\"\"\"\n",
        "  return augmented_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JHTutK09GRSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d5694e-d353-4f53-a774-16e8b25c9cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the contexts below, answer the query.\n",
            "\n",
            "  contexts:\n",
            "  languages, such as Chinese.\n",
            "In this technical report, we introduce Baichuan\n",
            "2, a series of large-scale multilingual language\n",
            "models. Baichuan 2 has two separate models,\n",
            "Baichuan 2-7B with 7 billion parameters and\n",
            "Baichuan 2-13B with 13 billion parameters. Both\n",
            "models were trained on 2.6 trillion tokens, which\n",
            "to our knowledge is the largest to date, more than\n",
            "double that of Baichuan 1 (Baichuan, 2023b,a).\n",
            "With such a massive amount of training data,\n",
            "Baichuan 2: Open Large-scale Language Models\n",
            "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\n",
            "Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\n",
            "Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\n",
            "Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\n",
            "Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\n",
            "adequate training of each word embedding. We\n",
            "have taken both these aspects into account. We\n",
            "have expanded the vocabulary size from 64,000\n",
            "in Baichuan 1 to 125,696, aiming to strike a\n",
            "balance between computational efficiency and\n",
            "model performance.\n",
            "Tokenizer V ocab Size Compression Rate ↓\n",
            "LLaMA 2 32,000 1.037\n",
            "Bloom 250,680 0.501\n",
            "ChatGLM 2 64,794 0.527\n",
            "Baichuan 1 64,000 0.570\n",
            "Baichuan 2 125,696 0.498\n",
            "Table 2: The vocab size and text compression rate of\n",
            "\n",
            "  query: How large is the baichuan2 vocabulary?\n"
          ]
        }
      ],
      "source": [
        "print(augment_prompt(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPNBZlRPGlDB",
        "outputId": "6e9909f1-a58b-4952-cbf6-06f9711df2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the given context, the Baichuan 2 vocabulary size is 125,696.\n"
          ]
        }
      ],
      "source": [
        "# 创建prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(query)\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpwaPwJteHz3"
      },
      "source": [
        "### 没有OPENAI api key怎么办 创建一个非openai的对话模型  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgKpiexG-AS_"
      },
      "source": [
        "\n",
        "1.   embedding模型  \n",
        "2.   chat模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExJrgFacesTo"
      },
      "outputs": [],
      "source": [
        "! pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvD4mIBCHKjG"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "model_name = \"sentence-transformers/sentence-t5-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzFKi_bQehhX"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJP5K7J5ehlB"
      },
      "outputs": [],
      "source": [
        "vectorstore_hf = Chroma.from_documents(documents=docs, embedding=embedding , collection_name=\"huggingface_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-SnVdrpfRR7"
      },
      "outputs": [],
      "source": [
        "result = vectorstore_hf.similarity_search(query ,k = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o2lU8qZfRR7"
      },
      "outputs": [],
      "source": [
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBT6ReBQfRR7"
      },
      "source": [
        "通过本地部署的模型进行交互"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i8TTksfT2K3r",
        "OZN6yzwi7J61",
        "lgABYtKp8_Ke",
        "ymUjel7-E-t1"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}